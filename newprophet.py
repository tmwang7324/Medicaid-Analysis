# -*- coding: utf-8 -*-
"""prophet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKABVtIfC_kcj66pXxqi1nyVnYAn2SXp
"""


import pandas as pd
df = pd.read_csv('df.csv')
df

df.dtypes

# split the data into training and testing data in order, then calculate the mae and r2 to see general performance

import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from prophet import Prophet

# assuming your dataframe df is defined
df['ds'] = pd.to_datetime(df['ds']) # convert ds to datetime

# create the train/test split
train_size = int(0.8 * len(df))  # 80% of data as training
df_train = df[:train_size]
df_test = df[train_size:]

# define and fit the model to the training data
model = Prophet()
model.fit(df_train)

# make predictions on the test set
forecast = model.predict(df_test.drop(columns='y'))

# evaluate the model using MAE and R2
mae = mean_absolute_error(df_test['y'], forecast['yhat'])
r2 = r2_score(df_test['y'], forecast['yhat'])

print('MAE: %.3f' % mae)
print('R2: %.3f' % r2)

# visualize the actual vs predicted values using matplotlib

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(df_test['ds'], df_test['y'], label='Actual')
plt.plot(df_test['ds'], forecast['yhat'], label='Predicted')
plt.xlabel('Date')
plt.ylabel('Value')
plt.title('Actual vs Predicted Values')
plt.legend()
plt.show()

#One way to detect overfitting is by comparing the error on the training set with the error on the test set. If the training error is significantly lower than the test error, it is a sign of overfitting.

import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, r2_score

# assuming your dataframe df is defined
df['ds'] = pd.to_datetime(df['ds']) # convert ds to datetime

# create the train/test split
train_size = int(0.8 * len(df))  # 80% of data as training
df_train = df[:train_size]
df_test = df[train_size:]

# define and fit the model to the training data
model = Prophet()
model.fit(df_train)

# make predictions on the test set
forecast_test = model.predict(df_test.drop(columns='y'))
forecast_train = model.predict(df_train.drop(columns='y'))

# evaluate the model using MAE and R2 for both train and test sets
mae_train = mean_absolute_error(df_train['y'], forecast_train['yhat'])
r2_train = r2_score(df_train['y'], forecast_train['yhat'])

mae_test = mean_absolute_error(df_test['y'], forecast_test['yhat'])
r2_test = r2_score(df_test['y'], forecast_test['yhat'])

print('Train MAE: %.3f' % mae_train)
print('Train R2: %.3f' % r2_train)

print('Test MAE: %.3f' % mae_test)
print('Test R2: %.3f' % r2_test)

# we see Train MAE: 551206.425
#Train R2: 0.955
#Test MAE: 1471636.807
#Test R2: 0.789    so it's overfitting

# make predictions for next 10 years

# create a future dataframe for 10 years
future = model.make_future_dataframe(periods=10*365)

# make predictions for the next 10 years
forecast = model.predict(future)

# plot the forecast
model.plot(forecast)
plt.show()
